{
  "articles": [
    {
      "path": "BioFB.html",
      "title": "BioFB",
      "author": [],
      "contents": "\n\nSlow-Paced Breathing in VR\n\n\nVOICE: FABIAN MUELLER | EXTRA: AMIRA BÜTTNER | PRODUCED: NEW MEDIA CENTER UNIVERSITY OF BASEL\n\n\n\n\n\n\n\n\nEngage in a guided, slow-paced breathing VR training designed to reduce stress. We develop VR calming and rewarding VR experiences that promote relaxation and stress relief through guided breathing exercises.\n\n\nAssessing the Acute Effects of Virtual Reality-Based Breathing Interventions on Stress: A Pilot Study\n\n\n\n\n\n\nHere is the flyer for our upcoming study, set to begin in October 2024. The official study website and the OSF preregistration will be launched by September.\n\n\n\nMore information coming soon.\n\n\n\n\n\n\n\n\n",
      "last_modified": "2024-07-31T09:42:48+02:00"
    },
    {
      "path": "contact.html",
      "title": "Hello.",
      "author": [],
      "contents": "\n\nFabian D. Mueller \n\n\nResearch Cluster Molecular and Cognitive Neurosciences \nDepartment of Biomedicine \nUniversity of Basel \nBirmannsgasse 8, 4055 Basel, Switzerland \n fabi.mueller@unibas.ch\n\n\n ORCID\n\n\n Google Scholar\n\n\n ResearchGate\n\n\n Twitter\n\n\n Download CV\n\n\n\n\n\n\n\n",
      "last_modified": "2024-07-31T09:42:48+02:00"
    },
    {
      "path": "echtzeit.html",
      "title": "Echtzeit",
      "author": [],
      "contents": "\n\nEchtzeit\n\n\nELEKTRONICS: JOSUA WEHNER | KEYS: SEVERIN SCHERRER | BASS: MARC ZÜST | DRUMS: FABIAN MUELLER\n\n\n\n\n\nWith our music quartet, Echtzeit, we aimed to harmonize the lively essence of acoustic music with the technical possibilities of electronic music.\nDrawing inspiration from jazz virtuosos such as Brad Mehldau and drummers like Jojo Mayers, we’ve crafted a unique sound that blends the rhythmic intensity of drum n bass with the complex harmonies of jazz.\n\n\n\n\n\n\n\n\nCheck out our sound on Spotify.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
      "last_modified": "2024-07-31T09:42:49+02:00"
    },
    {
      "path": "fearless.html",
      "title": "Fearless Speech",
      "author": [],
      "contents": "\n\nFearless Speech\n\n\nA Virtual Reality Gaze Exposure Treatment to Reduce Fear of Public Speaking\n\n\n\n\n\nWe developed a stand-alone gaze exposure treatment in virtual reality (VR) to test the potential of reducing gaze avoidance to reduce anxiety. The gaze exposure trained the maintenance of eye contact with audiences across progressively challenging social scenarios.\n\n\nVR Gaze Exposure Application\n\n\n\n\n\n\n\n\nThe figure displays a classroom (A), proximity (B), and lecture hall scenario (C) used in the gaze exposure treatment. From left to right, the social scenario increases in difficulty, with differences in audience size, proximity, and facial expressions. A green arrow indicates a target person for maintaining eye contact. After successfully holding eye contact with the target person over a predefined time as measured by gaze tracking, the arrow switched to the next target person to prompt a change of eye contact. Each level was repeated until the overall eye contact maintenance exceeded a predefined time threshold ranging from 56 to 96 seconds, depending on the level, and the user indicated low anxiety (i.e., < 3) on a state anxiety rating.\n\n\nSee here how this works…\n\n\n\n\nStudy Design\n\n\nIn a single-blind, randomized controlled trial, we evaluated the effectiveness of exposure to gaze in reducing state anxiety. Eighty-nine adult participants with subclinical PSA were assigned to either a gaze exposure treatment or a control group.\n\n\n\n\n\nAssessments occurred at baseline, following a one-hour acute intervention (Post 1), and three to five weeks after a two-weeks home treatment (Post 2). At each assessment, participants conducted a public speaking test (PST, see below). After a baseline PST, the participants of the treatment group underwent the gaze exposure treatment in VR for 3 × 20 min, while the participants of the control group explored virtual scenarios without social content. The second PST followed the procedure as the first one. Subsequently, the treatment group completed 9 × 20 min home treatment sessions with the gaze exposure app, while the control group did not receive any task. Three to five weeks after treatment completion of the treatment group, a third public speaking test was conducted.\n\n\nPublic Speaking Test\n\n\nWe tested treatment effects in a real-life public speaking situation. The primary Outcome was state anxiety during public speaking, assessed using the Subjective Units of Distress Scale.\nThe main secondary outcome was gaze avoidance during public speaking, as measured by eye-tracking.\n\n\n\n\n\n\nThe figure displays the view of the participant during the real-life public speaking test as recorded through a mobile eye-tracking system The public speaking test (PST) comprised ten minutes of semi-improvised speeches in front of an evaluation committee. Before the first and after every speech, participants indicated their state anxiety level on the Subjective Units of Distress Scale. During the speeches, we used eye tracking and a face detection algorithm to quantify gaze avoidance as the relative dwell time off faces of any committee members during public speaking. Here, A video frame is shown, recorded from the world camera of a mobile eye tracking system, representing a participant’s perspective during the public speaking test.\n\n\nResults\n\n\n\n\n\nThe figure displayes the effects of the gaze exposure treatment on state anxiety and gaze avoidance during public speaking. Linear mixed models’ analyses suggested no reduction regarding state anxiety or the relative dwell time off faces during the public speaking test in the treatment group immediately after 1-h gaze exposure (Post 1) compared to the control group. However, there was a beneficial effect on state anxiety (cohens d’ = 1.07) and gaze avoidance (cohens d’ = - 0.97) after additional home treatment (9 × 20 min), as evident by the difference between the groups one month after the intervention (Post 2).\n\n\nImplications\n\n\n• A two-week VR gaze exposure reduces state anxiety during public speaking. \n• VR gaze exposure requires no exposure to real people or any verbal interaction.\n• VR exposure apps offer accessible self-help tools at a low initiation threshold. \n\n\nPublication\n\n\nVirtual reality gaze exposure treatment reduces state anxiety during public speaking in individuals with public speaking anxiety: A randomized controlled trial @ Journal of Anxiety Disorders Reports\n\n\n\nBernhard Fehlmann1,5,\nFabian D Mueller1,5,\nNan Wang1,5,\nMerle K Ibach1,5,\nThomas Schlitt2,5,\nDorothée Bentz1,5,\nAnja Zimmer1,5,\nAndreas Papassotiropoulos2,3,4,5,\nDominique JF de Quervain1,4,5\n\nBernhard Fehlmann & Fabian D Mueller contributed equally to this work\n\n\nDivision of Cognitive Neuroscience, Department of Biomedicine, University of Basel, Birmannsgasse 8, Basel 4055, Switzerland\n\n\nDivision of Molecular Neuroscience, Department of Biomedicine, University of Basel, Basel 4055, Switzerland\n\n\nLife Sciences Training Facility, Department Biozentrum, University of Basel, Basel 4056, Switzerland\n\n\nUniversity Psychiatric Clinics, University of Basel, Basel 4002, Switzerland\n\n\nResearch Cluster Molecular and Cognitive Neurosciences, University of Basel, Basel 4055, Switzerland\n\n\n\nPoster\n\n\nThis work was presented at the 26th Annual CyberPsychology, CyberTherapy & Social Networking Conference\n(CYPSY26). It was honored with the 2023\nYoung Minds Research Award.\n\n\n\n\n\nClick the poster to view the full PDF\n\n\n\nIn the News\n\n\n• SwissInfo\n• Watson\n• Handelszeitung\n• Bilanz\n• Blick\n\n\n\n\n\n\n",
      "last_modified": "2024-07-31T09:42:49+02:00"
    },
    {
      "path": "hallucinations.html",
      "title": "Hallucinations",
      "author": [],
      "contents": "\n\nHow does our brain shape what we see?\n\n\n\nIn this video, we explains how scientists think hallucinations occur in the brain by introducing the Predictive Processing framework.\n\n\n\n\n\nIDEA: FABIAN MUELLER | PRODUCED: NEW MEDIA CENTER\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLearn more about our ongoing study, where we use VR to induce hallucination-like phenomena to disentangle the perceptual decision-making factors that contribute to the emergence of visual disturbances.\n\n\n\n\n\n\n\n",
      "last_modified": "2024-07-31T09:42:50+02:00"
    },
    {
      "path": "hoffman.html",
      "title": "Hoffman's First Ride",
      "author": [],
      "contents": "\n\nHoffman’s First Ride\n\n\nInteractive installation presented at the Rausch - Extase - Rush Exhibition,  Historical Museum in Bern.\n\n\n\n\n\n\nIDEA AND TECHNICAL IMPLEMENTATION: THOMAS MOLL | FILM: FABIAN MÜLLER & RAMONDS JERMAKS |\nRESPONSIBILITY: EXPOFORUM GMBH | CONSTRUCTION IMPLEMENTATION: HISTORICAL MUSEUM BERN\n\n\nPartnering with Raimonds Jermaks (Symmetric-Vision), we created an interactive video installation that brings to life Albert Hoffmann’s first LSD trip, an event that occurred quite by accident in his laboratory in Basel. The installation invites participants to mount a bicycle, the pedalling of which triggers a captivating video, in which the visual effects of LSD gradually increase over time. The video is based on the path that Albert Hoffman took on his historic bicycle ride home, a journey during which he became the first human to ever experience the profound effects of LSD. \n\n\n\n\n\n\n\n\n\nHere’s the full video …\n\n\n\n\nDisclaimer: There is much more to psychedelic experiences than just funky visuals. We do not claim that this visualization captures the nature of such.\n\n\n\n\n\n\n",
      "last_modified": "2024-07-31T09:42:50+02:00"
    },
    {
      "path": "index.html",
      "title": "Hello.",
      "author": [],
      "contents": "\n\n\n\n\n\n\n\n\n\nI am a cognitive psychologist pursuing a PhD in Cognitive Neuroscience at the University of Basel, Switzerland and Yale University, USA. My broad aim is to develop straightforward and scalable treatment interventions for mental health, using the potential of contemporary technologies, with a focus on immersive technologies like AR and VR.\n\n\nAs a personal time capsule, this website showcases my research and artistic projects. \n\n\n\n",
      "last_modified": "2025-03-20T13:15:54+01:00"
    },
    {
      "path": "LiveSet.html",
      "title": "LiveSet",
      "author": [],
      "contents": "\n\n\n\n\n\n\n\nWith Noe Weigl, who introduced me to and taught me a lot about electronic music production, we checked out how we could perform semi-life electronic music using analog drum machines (Analog Rytm), analog (Novation Peak, Sub 37) and virtual (mostly Diva & friends) Synthesizers, and Ableton Live.\n\n\n\nNOE WEIGL & FABIAN MUELLER\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBrüders et al. · LiveSet Noe & Faby @ Radiologisch\n\n\n\n\n\n\n\n\n",
      "last_modified": "2024-07-31T09:42:51+02:00"
    },
    {
      "path": "MRI.html",
      "title": "BioFB",
      "author": [],
      "contents": "\n\nVR Training for MRI Anxiety\n\n\nMRI SCANNER RECORDED AT UNIVERSITY HOSPITAL BASEL | RECORDED BY FABIAN MUELLER\n\n\n\n\n\nImmerse yourself in a virtual MRI environment designed to alleviate fears and enhance comfort. Our VR simulations offer a safe, controlled space to acclimate to MRI procedures, enabling a more relaxed and confident experience when it counts.\n\n\n\nMore information coming soon. Stay tuned!\n\n\n\n\n\n\n\n\n",
      "last_modified": "2024-07-31T09:42:52+02:00"
    },
    {
      "path": "music.html",
      "title": "Hoffman's First Ride",
      "author": [],
      "contents": "\n\nStrollology\n\n\nJOSHUA JAEGER | LOUIS OBERLI | LUKAS HUBER | ROMANO MADDALENA | FABIAN MUELLER\n\n\n\n\n\n\n\nBrüders et al. · Ergehungen\n\n\n\nStrollology is contemplative cultural and aesthetic method to raise awareness of the conditions of perception of the environment and to expand environmental perception. It is based on experimental reflexive walks and aesthetic interventions. Together with my friends, we often go for expanded sound-walks, in which we share our recent musical discoveries. Each of us puts together a short music set with his/her most recent treasure discoveries. We all press play at the exactly same time (this usually takes a few attempts — but exact synchronization is absolutely essential!), and then we walk and let the music guide our mind while enjoying the beauty of nature. \n\n\n\n\n\n\n",
      "last_modified": "2024-07-31T09:42:52+02:00"
    },
    {
      "path": "plexus.html",
      "title": "Sound Figures",
      "author": [],
      "contents": "\n\nSound Figures\n\n\nAudio Engineer Hannes Kumke, who recorded and mastered some tracks of our album Accretion Disk, visualized our track Plexus using water and light to reflect what our track looks like. The idea is based on Alexander Lauterwasser’s Kymatik-Cymatics - Sound Figures.\n\n\n\nELEKTRONICS: JOSUA WEHNER | DRUMS: FABIAN MUELLER | MASTERED AND VISUALIZED: HANNES KUMKE\n\n\n\n\n\n\n\n\n\n\n\n",
      "last_modified": "2024-07-31T09:42:53+02:00"
    },
    {
      "path": "psychosis.html",
      "title": "Altered Perception in Psychosis",
      "author": [],
      "contents": "\n\n“In our opinion, prevention of psychosis in the pre-psychotic precursor stages is possible.”\n\n\n— Gerd Huber, 1987\n\n\nPsychosis is characterized by hallucinations and delusions (fixed false beliefs). However, for most patients, the onset of full-blown psychotic symptoms is preceded by subtler changes in perception, thought and beliefs. Take a closer look at Mona. Do you notice anything unusual? \nAlthough the experience of looking at the Mona Lisa is an obvious illustration, consider how changes in perception can influence your feelings and thoughts about the world. But beyond fostering empathy for those affected, one key question we care about is whether we can identify individuals at risk for psychosis before the onset of full-blown symptoms. To address this question, we investigate the prevalence and mechanisms of perceptual distortions in psychosis.\n\n\n\n\n\nSimulating Psychosis-Like Visual Distortions in VR\n\n\n\n\n\n Visual distortions in individuals at high risk for psychosis are associated with a poor prognosis, including conversion to full-blown psychosis. We have collaborated with Symmetric Vision, a visual artist, to simulate some of the most commonly reported visual distortions in virtual reality (VR). Here we display a set of six visual distortions, comprising alterations in brightness, metachromopsia (color alterations), visual patterns, pseudo-movement, and peripheral shadows.\n\n\n\n\n\nOn the clinical side …\n\n\nBased on these VR simulations, we have developed a Visual Distortion Screener to show these simulations both on a computer screen and in VR. The screener is designed to assess experiences of visual distortions in patients on the psychosis spectrum by providing visual illustrations. Unlike earlier measures, our novel semi-structured interview does not rely on verbal descriptions of visual phenomena, which could hinder accurate reporting of experienced symptoms.\n\n\n\n\n\n\nAfter showing the videos, the screener asks the user to briefly describe the visual distortion to confirm that it was correctly recognized. Subsequently, the screener asks, “Have you seen something like this in your everyday life?” If the participant endorses having experienced the distortion, they will be asked a set of questions probing the frequency, duration, intrusiveness, and distress of the distortion within the past month and during the “worst time.”\n\n\nOn the mechanistic side …\n\n\nWe are investigating what drives visual disturbances more broadly, including the occurrence of hallucinations. Hallucinations are classically defined as perception-like experiences that occur without an external stimulus. Hallucinations are not limited to psychotic disorders but also occur in the general population, suggesting a continuum from attenuated to more severe forms of hallucinatory experiences. We have developed three novel perceptual decision-making tasks in VR that are aimed at inducing hallucination-like percepts and disentangling the perceptual decision-making factors that contribute to the emergence of visual disturbances. These tasks are:\n\n\nNaïve Detection Task, probing participants’ spontaneous susceptibility to false perceptions and visual sensitivity when only knowing that there may be visual changes.\n\n\nTwo-Interval Forced-Choice (2-IFC) Task, isolating visual sensitivity from cognitive bias using a psychophysical procedure.\n\n\nPerceptual Uncertainty Task, inducing false percepts by varying perceptual uncertainty over time.\n\n\n\n\n\nWith the Naïve Detection Task, we assess spontaneous false percepts and perceptual sensitivity in detecting an unknown visual distortion in naturalistic VR scenarios. To do this, participants are placed in a VR environment for two minutes and their task is to report, as soon as they believe that something has changed. Importantly, participants do not know about the following task structure. For the initial 30 seconds of the task, no changes are introduced to capture potential false (imagined) change detections. Then, one of the visual distortion is faded in gradually over 90 seconds to measure at what itensity the target distortions is correctly identified. Throughout this task, we record all button presses, and corresponding verbal reports as well as gaze behavior as a measure of visual exploration.\n\n\nA drawback of the Naïve Detection Task is that, that we cannot discriminate perceptual sensitivity from change blindness, memory effects or cognitive response tendencies, like differing decision criteria. And these will likely contribute to the assessment of sensitivity with this task. So, to rule out such confounding variables, in the next task, we determine sensitivity in terms of perceptual thresholds.\n\n\n\n\n\nThe Two-Interval Forced-Choice (2-IFC) Task is aimed at isolating visual sensitivity from cognitive bias using a psychophysical procedure. In this task, we present participants with a distortion either in the first or the second half of a trial. After each trial, participants have to decide whether the change was present in the first or the second half. This task is combined with an adaptive staircase procedure that estimates the threshold after each response and presents this estimated intensity in the next trial. This process is repeated until, after around 50 trials, we obtain relatively robust threshold estimates as a cleaner measure of perceptual sensitivity towards the visual distortions.\n\n\n\n\n\nThe Perceptual Uncertainty Task was designed to induce hallucination-like false percepts through perceptual ambiguity in a more natural and continuous VR experience. Over a duration of 10 minutes, participants will be presented with distortions at their threshold, slightly above or below, as well as in distortion-absent trials. The task for the participants is to press a button whenever they believe they perceive a change. Although participants won’t be aware of the different trials, we will collect response rates across the different stimulus intensities, as well as in the distortion-absent trials.\n\n\n\nAdditionally, we also want to look into implicitly learned frequency expectations. Therefore, unknown to the participants, we’re using a block design. In the first 5 minutes in VR, there are many distortions that can be clearly seen. In the second 5 minutes, there are much fewer detectable distortions. And we want to investigate if or how perceiving a higher frequency of distortions in the first 5 minutes carries over to the second block, where much fewer perceivable distortions are shown.\n\n\nOngoing Studies\n\n\nWe are currently conducting two studies to investigate the perceptual factors underlying visual disturbances. One study involves general population participants, and the other includes participants at high risk for psychosis. With our three novel perceptual decision-making tasks in VR, we aim to dissect participant-specific sensitivity to psychosis-like visual distortions and cognitive bias towards false percepts. We are further examining gaze behavior in relation to our behavioral task outcomes and relating these outcomes to self-reported psychosis-like experiences in healthy participants or symptoms in patients, respectively.\n\n\n\n\nBelow is the flyer for our ongoing general population study. Visit the\nstudy website\nand the\nOSF preregistration\nfor more details.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
      "last_modified": "2024-07-31T09:42:53+02:00"
    },
    {
      "path": "scismile.html",
      "title": "SciSmile",
      "author": [],
      "contents": "\n\nSciSmile\n\n\nWith SciSmile, we aim to offer a scientifically-grounded, straightforward tool to promote psychological well-being and resilience in the face of stress.\n\n\n\n\n\n\nSciSmile is a smartphone application designed to improve well being and reduce stress through the power of smiling. SciSmile simulates reciprocal smiling interactions using videos of smiling people. The app tracks when users smile and triggers a responsive video of a person smiling back, thereby creating positive social feedback.\n\n\n\n\n\n\n\n\nCan it effectively reduce stress?\n\n\nWe launched the SciSmile app on both the Android and Apple App stores in 2021 to assess its appeal and effectiveness in the general population. The primary focus was to determine if such an intervention could effectively reduce stress levels and elevate mood. Preliminary results from our anonymous online pilot study in Switzerland suggest that a two-week training with SciSmile has strong potential for reducing stress and enhancing mood.\nOne year later, we launched SciMem, a simple smartphone memory game app. Interestingly, we found comparable effects.\nThus, it remains to be determined whether these beneficial effects are primarily the result of the apps or if they could be attributed to other factors such as placebo effects or regression to the mean. To answer this question, we are conducting a three-arm RCT to test the efficacy of SciSmile and SciMem, compared to a waitlist control.\n\n\n\n\n\nExample Screenshots from the SciSmile & SciMem App. Left: SciSmile; Right: SciMem.\n\n\nComparison of the efficacy of two smartphone apps on stress:  a three arm RCT\n\n\n\n\n\n\nHere is the flyer for our upcoming study, set to begin in September 2024. The official study website and the OSF preregistration will be launched by August.\n\n\n\n\n\n\n",
      "last_modified": "2024-07-31T09:42:54+02:00"
    },
    {
      "path": "stages.html",
      "title": "Stages",
      "author": [],
      "contents": "\n\nScenic Architecture\n\n\nI am part of Pantasia, a raving DIY collective designing experiences at music and art festivals. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
      "last_modified": "2024-07-31T09:42:54+02:00"
    }
  ],
  "collections": []
}
