{
  "articles": [
    {
      "path": "about.html",
      "title": "The best way to learn is to experience.",
      "description": "Irgendöbis laber laber",
      "author": [],
      "contents": "\n\n\n\n",
      "last_modified": "2024-05-13T12:34:02+02:00"
    },
    {
      "path": "BioFB.html",
      "title": "BioFB",
      "author": [],
      "contents": "\n\nSlow-Paced Breathing in VR\n\n\nVOICE: FABIAN MUELLER | EXTRA: AMIRA BÜTTNER | PRODUCED: NEW MEDIA CENTER UNIVERSITY OF BASEL\n\n\n\nEngage in a guided, slow-paced breathing VR training designed to reduce stress. Our VR simulations provide a calming and rewarding VR experiences that promotes relaxation and stress relief through guided breathing exercises.\n\n\n\nMore information coming soon.\n\n\n\n\n\n\n\n\n",
      "last_modified": "2024-05-27T15:51:26+02:00"
    },
    {
      "path": "contact.html",
      "title": "Hello.",
      "author": [],
      "contents": "\n\nFabian D. Mueller \n\n\nResearch Cluster Molecular and Cognitive Neurosciences \nDepartment of Biomedicine \nUniversity of Basel \nBirmannsgasse 8, 4055 Basel, Switzerland \n fabi.mueller@unibas.ch\n\n\n ORCID\n\n\n Google Scholar\n\n\n ResearchGate\n\n\n Twitter\n\n\n\n\n\n\n\n",
      "last_modified": "2024-06-30T16:37:20+02:00"
    },
    {
      "path": "echtzeit.html",
      "title": "Echtzeit",
      "author": [],
      "contents": "\n\nEchtzeit\n\n\nELEKTRONICS: JOSUA WEHNER | KEYS: SEVERIN SCHERRER | BASS: MARC ZÜST | DRUMS: FABIAN MUELLER\n\n\n\n\n\nWith our music quartet, Echtzeit, we aimed to harmonize the lively essence of acoustic music with the technical possibilities of electronic music.\nDrawing inspiration from jazz virtuosos such as Brad Mehldau and drummers like Jojo Mayers, we’ve crafted a unique sound that blends the rhythmic intensity of drum n bass with the complex harmonies of jazz.\n\n\n\n\n\n\n\n\nCheck out our sound on Spotify.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
      "last_modified": "2024-05-29T18:32:33+02:00"
    },
    {
      "path": "fearless.html",
      "title": "Fearless Speech",
      "author": [],
      "contents": "\n\nFearless Speech\n\n\nA Virtual Reality Gaze Exposure Treatment to Reduce Fear of Public Speaking\n\n\n\n\n\nWe developed a stand-alone gaze exposure treatment in virtual reality (VR) to test the potential of reducing gaze avoidance to reduce anxiety. The gaze exposure trained the maintenance of eye contact with audiences across progressively challenging social scenarios.\n\n\nVR Gaze Exposure Application\n\n\n\n\n\n\n\n\nThe figure displays a classroom (A), proximity (B), and lecture hall scenario (C) used in the gaze exposure treatment. From left to right, the social scenario increases in difficulty, with differences in audience size, proximity, and facial expressions. A green arrow indicates a target person for maintaining eye contact. After successfully holding eye contact with the target person over a predefined time as measured by gaze tracking, the arrow switched to the next target person to prompt a change of eye contact. Each level was repeated until the overall eye contact maintenance exceeded a predefined time threshold ranging from 56 to 96 seconds, depending on the level, and the user indicated low anxiety (i.e., < 3) on a state anxiety rating.\n\n\nSee here how this works…\n\n\n\n\nStudy Design\n\n\nIn a single-blind, randomized controlled trial, we evaluated the effectiveness of exposure to gaze in reducing state anxiety. Eighty-nine adult participants with subclinical PSA were assigned to either a gaze exposure treatment or a control group.\n\n\n\n\n\nAssessments occurred at baseline, following a one-hour acute intervention (Post 1), and three to five weeks after a two-weeks home treatment (Post 2). At each assessment, participants conducted a public speaking test (PST, see below). After a baseline PST, the participants of the treatment group underwent the gaze exposure treatment in VR for 3 × 20 min, while the participants of the control group explored virtual scenarios without social content. The second PST followed the procedure as the first one. Subsequently, the treatment group completed 9 × 20 min home treatment sessions with the gaze exposure app, while the control group did not receive any task. Three to five weeks after treatment completion of the treatment group, a third public speaking test was conducted.\n\n\nPublic Speaking Test\n\n\nWe tested treatment effects in a real-life public speaking situation. The primary Outcome was state anxiety during public speaking, assessed using the Subjective Units of Distress Scale.\nThe main secondary outcome was gaze avoidance during public speaking, as measured by eye-tracking.\n\n\n\n\n\n\nThe figure displays the view of the participant during the real-life public speaking test as recorded through a mobile eye-tracking system The public speaking test (PST) comprised ten minutes of semi-improvised speeches in front of an evaluation committee. Before the first and after every speech, participants indicated their state anxiety level on the Subjective Units of Distress Scale. During the speeches, we used eye tracking and a face detection algorithm to quantify gaze avoidance as the relative dwell time off faces of any committee members during public speaking. Here, A video frame is shown, recorded from the world camera of a mobile eye tracking system, representing a participant’s perspective during the public speaking test.\n\n\nResults\n\n\n\n\n\nThe figure displayes the effects of the gaze exposure treatment on state anxiety and gaze avoidance during public speaking. Linear mixed models’ analyses suggested no reduction regarding state anxiety or the relative dwell time off faces during the public speaking test in the treatment group immediately after 1-h gaze exposure (Post 1) compared to the control group. However, there was a beneficial effect on state anxiety (cohens d’ = 1.07) and gaze avoidance (cohens d’ = - 0.97) after additional home treatment (9 × 20 min), as evident by the difference between the groups one month after the intervention (Post 2).\n\n\nImplications\n\n\n• A two-week VR gaze exposure reduces state anxiety during public speaking. \n• VR gaze exposure requires no exposure to real people or any verbal interaction.\n• VR exposure apps offer accessible self-help tools at a low initiation threshold. \n\n\nPublication\n\n\nVirtual reality gaze exposure treatment reduces state anxiety during public speaking in individuals with public speaking anxiety: A randomized controlled trial @ Journal of Anxiety Disorders Reports\n\n\n\nBernhard Fehlmann1,5,\nFabian D Mueller1,5,\nNan Wang1,5,\nMerle K Ibach1,5,\nThomas Schlitt2,5,\nDorothée Bentz1,5,\nAnja Zimmer1,5,\nAndreas Papassotiropoulos2,3,4,5,\nDominique JF de Quervain1,4,5\n\nBernhard Fehlmann & Fabian D Mueller contributed equally to this work\n\n\nDivision of Cognitive Neuroscience, Department of Biomedicine, University of Basel, Birmannsgasse 8, Basel 4055, Switzerland\n\n\nDivision of Molecular Neuroscience, Department of Biomedicine, University of Basel, Basel 4055, Switzerland\n\n\nLife Sciences Training Facility, Department Biozentrum, University of Basel, Basel 4056, Switzerland\n\n\nUniversity Psychiatric Clinics, University of Basel, Basel 4002, Switzerland\n\n\nResearch Cluster Molecular and Cognitive Neurosciences, University of Basel, Basel 4055, Switzerland\n\n\n\nPoster\n\n\nThis work was presented at the 26th Annual CyberPsychology, CyberTherapy & Social Networking Conference\n(CYPSY26). It was honored with the 2023\nYoung Minds Research Award.\n\n\n\n\n\nClick the poster to view the full PDF\n\n\n\nIn the News\n\n\n• SwissInfo\n• Watson\n• Handelszeitung\n• Bilanz\n• Blick\n\n\n\n\n\n\n",
      "last_modified": "2024-05-29T20:24:18+02:00"
    },
    {
      "path": "hallucinations.html",
      "title": "Hallucinations",
      "author": [],
      "contents": "\n\nHow does our brain shape what we see?\n\n\n\nThis video explains how scientists think hallucinations occur in the brain by introducing the Predictive Processing framework. The video was produced by the New Media Center  of the University of Basel.\n\n\n\n\n\n\n\nIDEA: FABIAN MUELLER | PRODUCED: NEW MEDIA CENTER\n\n\n\n\n\n\n\n\n",
      "last_modified": "2024-07-01T14:20:42+02:00"
    },
    {
      "path": "hoffman.html",
      "title": "Hoffman's First Ride",
      "author": [],
      "contents": "\n\nHoffman’s First Ride\n\n\nInteractive installation presented at the Rausch - Extase - Rush Exhibition,  Historical Museum in Bern.\n\n\n\n\n\nPartnering with Raimonds Jermaks (Symmetric-Vision), we created an interactive video installation that brings to life Albert Hoffmann’s first LSD trip, an event that occurred quite by accident in his laboratory in Basel. The installation invites participants to mount a bicycle, the pedalling of which triggers a captivating video, in which the visual effects of LSD gradually increase over time. \n\n\n\n\n\n\nThe video is based on the path that Albert Hoffman took on his historic bicycle ride home, a journey during which he became the first human to ever experience the profound effects of LSD.\n\n\nHere’s the full video …\n\n\nDisclaimer: There is much more to psychedelic experiences than just funky visuals. We do not claim that this visualization captures the nature of such.\n\n\n\n\n\n\n",
      "last_modified": "2024-06-21T21:03:21+02:00"
    },
    {
      "path": "index.html",
      "title": "Hello.",
      "author": [],
      "contents": "\n\n\n\n\nI am a cognitive psychologist pursuing a PhD in Cognitive Neuroscience at the University of Basel, Switzerland and Yale University, USA. My broad aim is to develop straightforward and scalable treatment interventions for mental health, using the potential of contemporary technologies, with a focus on immersive technologies like AR and VR.\nWith a background in Psychology, Philosophy, and Neuroscience, I am most fundamentally interested in understanding how our brains constructs our phenomenological reality.  To understand this, I study perceptual processing in individuals who perceive reality differently (i.e., psychosis). My approach embeds perceptual decision-making in virtual reality simulations of altered perception.\n\n\n\n\n\n\n",
      "last_modified": "2024-07-01T13:38:56+02:00"
    },
    {
      "path": "LiveSet.html",
      "title": "LiveSet",
      "author": [],
      "contents": "\n\n\n\n\n\n\n\nWith Noe Weigl, who introduced me to and taught me a lot about electronic music production, we checked out how we could perform semi-life electronic music using analog drum machines (Analog Rytm), analog (Novation Peak, Sub 37) and virtual (mostly Diva & friends) Synthesizers, and Ableton Live.\n\n\n\nNOE WEIGL & FABIAN MUELLER\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBrüders et al. · LiveSet Noe & Faby @ Radiologisch\n\n\n\n\n\n\n\n\n",
      "last_modified": "2024-05-29T18:28:14+02:00"
    },
    {
      "path": "MRI.html",
      "title": "BioFB",
      "author": [],
      "contents": "\n\nVR Training for MRI Anxiety\n\n\nMRI SCANNER RECORDED AT UNIVERSITY HOSPITAL BASEL | RECORDED BY FABIAN MUELLER\n\n\n\n\n\nImmerse yourself in a virtual MRI environment designed to alleviate fears and enhance comfort. Our VR simulations offer a safe, controlled space to acclimate to MRI procedures, enabling a more relaxed and confident experience when it counts.\n\n\n\nMore information coming soon.\n\n\n\n\n\n\n\n\n",
      "last_modified": "2024-07-01T13:46:09+02:00"
    },
    {
      "path": "music.html",
      "title": "Hoffman's First Ride",
      "author": [],
      "contents": "\n\nStrollology\n\n\nJOSHUA JAEGER | LOUIS OBERLI | LUKAS HUBER | ROMANO MADDALENA | FABIAN MUELLER\n\n\n\n\n\n\n\nBrüders et al. · Ergehungen\n\n\n\nStrollology is contemplative cultural and aesthetic method to raise awareness of the conditions of perception of the environment and to expand environmental perception. It is based on experimental reflexive walks and aesthetic interventions. Together with my friends, we often go for expanded sound-walks, in which we share our recent musical discoveries. Each of us puts together a short music set with his/her most recent treasure discoveries. We all press play at the exactly same time (this usually takes a few attempts — but exacts synchronization is absolutely essential!), and then we walk and let the music guide our mind while enjoying the beauty of nature. \n\n\n\n\n\n\n\n\n\n\n",
      "last_modified": "2024-05-27T15:41:16+02:00"
    },
    {
      "path": "plexus.html",
      "title": "Sound Figures",
      "author": [],
      "contents": "\n\nSound Figures\n\n\nAudio Engineer Hannes Kumke, who recorded and mastered some tracks of our album Accretion Disk, visualized our track Plexus using water and light to reflect what our track looks like. The idea is based on Alexander Lauterwasser’s Kymatik-Cymatics - Sound Figures.\n\n\n\nELEKTRONICS: JOSUA WEHNER | DRUMS: FABIAN MUELLER | MASTERED AND VISUALIZED: HANNES KUMKE\n\n\n\n\n\n\n\n\n\n\n\n",
      "last_modified": "2024-05-29T19:23:48+02:00"
    },
    {
      "path": "psychosis.html",
      "title": "Altered Perception in Psychosis",
      "author": [],
      "contents": "\n\n\n\n\n\n\n\n“In our opinion, prevention of psychosis in the pre-psychotic precursor stages is possible.”\n\n\n— Gerd Huber, 1987\n\n\nPsychosis is characterized by hallucinations and delusions (fixed false beliefs). However, for most patients, the onset of full-blown psychotic symptoms is preceded by subtler changes in perception, thought and beliefs, which seem to be attenuated forms of hallucinations and delusions, respectively. The question remains whether these experiences in psychosis-prone individuals are fundamentally driven by perceptual anomalies or higher-order cognitive processes.\n\n\n\nTake a closer look at Mona. Do you notice anything unusual? What does that do to you? \n\n\n\n\n\n\n\nVisual distortions in individuals at high risk for psychosis are associated with a poor prognosis, including conversion to full-blown psychosis. We have collaborated with Symmetric Vision, a visual artist, to replicate some of the most commonly reported visual distortions in virtual reality (VR). These include changes in clarity, brightness, shape, color, motion, and persistent patterns.\n\n\n\n\n\nVisual distortions simulations, that we generated based on 360° videos recorded in Basel, Switzerland in various settings of daily life (e.g., gym, park, and restaurant) and openly available online materials (www.eso.org). These videos were provided to a visual artist, along with descriptions of visual distortions commonly experienced by patients on the psychosis spectrum. Psychotic-like visual distortions were superimposed onto the 360° base materials across different VR scenarios. Here we display a set of six visual distortions, comprising alterations in brightness, metachromopsia (color alterations), visual patterns, pseudo-movement, and peripheral shadows.\n\n\nOn the clinical side, we have developed a Visual Distortion Screener to show these simulations both on a computer screen and in VR. This screener is designed to assess experiences of visual distortions in patients on the psychosis spectrum by providing visual illustrations of these distortions. Unlike earlier measures, our novel semi-structured interview does not rely on verbal descriptions of visual phenomena, which could hinder accurate reporting of experienced symptoms.\n\n\n\n\n\n\nAfter showing the videos, the participant will be asked to briefly describe the visual distortion that they saw to confirm that they identified the target distortion and then asked, “Have you seen something like this in your everyday life?” If the participant endorses having experienced the distortion, they will be asked a set of questions probing the frequency, duration, intrusiveness, and distress of the distortion within the past month and during the “worst time”.\n\n\nPredictive Processing\n\n\nAccording to the Predictive Coding hypothesis of psychosis, perceptual disturbances are thought to arise from aberrant perceptual inference due to imprecise integration of internal predictions and incoming sensory data4,15,16,25,53–55. However, different research groups have targeted specific mechanisms across the cortical hierarchy, providing evidence for each proposed mechanism. It remains inconclusive whether visual disturbances originate from abnormal perceptual sensitivity or heightened sensory expectations or both2.\n\n\n\n\n\n\n\n\nAdapted from: Sterzer, P., et al., (2018). Biological psychiatry.\n\n\n\n\n\nThe Naïve Detection Task is designed to assess spontaneous false percepts and perceptual sensitivity in detecting an unknown visual distortion in naturalistic VR scenarios. In each VR scenario, participants are first instructed to explore their environment for 20 seconds and to verbally describe what they observe. After this baseline exploration, participants are instructed that the upcoming task assesses their ability to detect changes in their environment. They are told to press a button on the VR controller as soon as they believe that something is changed. After pressing the button, they should verbally describe the observed change in two to three words. Importantly, participants do not know about the following task structure. For the initial 30 seconds of the task, no changes are introduced to capture potential false detections. Then, the target distortion is faded in gradually over 90 seconds. Throughout this total duration of 120 seconds, all button presses, and corresponding verbal reports as well as gaze behavior are recorded by the VR device.\n\n\n\n\n\nThe Naïve Detection Task is designed to assess spontaneous false percepts and perceptual sensitivity in detecting an unknown visual distortion in naturalistic VR scenarios. In each VR scenario, participants are first instructed to explore their environment for 20 seconds and to verbally describe what they observe. After this baseline exploration, participants are instructed that the upcoming task assesses their ability to detect changes in their environment. They are told to press a button on the VR controller as soon as they believe that something is changed. After pressing the button, they should verbally describe the observed change in two to three words. Importantly, participants do not know about the following task structure. For the initial 30 seconds of the task, no changes are introduced to capture potential false detections. Then, the target distortion is faded in gradually over 90 seconds. Throughout this total duration of 120 seconds, all button presses, and corresponding verbal reports as well as gaze behavior are recorded by the VR device.\n\n\n\n\n\nThe Naïve Detection Task is designed to assess spontaneous false percepts and perceptual sensitivity in detecting an unknown visual distortion in naturalistic VR scenarios. In each VR scenario, participants are first instructed to explore their environment for 20 seconds and to verbally describe what they observe. After this baseline exploration, participants are instructed that the upcoming task assesses their ability to detect changes in their environment. They are told to press a button on the VR controller as soon as they believe that something is changed. After pressing the button, they should verbally describe the observed change in two to three words. Importantly, participants do not know about the following task structure. For the initial 30 seconds of the task, no changes are introduced to capture potential false detections. Then, the target distortion is faded in gradually over 90 seconds. Throughout this total duration of 120 seconds, all button presses, and corresponding verbal reports as well as gaze behavior are recorded by the VR device.\n\n\n\n\n\n\n",
      "last_modified": "2024-05-30T18:18:24+02:00"
    },
    {
      "path": "scismile.html",
      "title": "SciSmile",
      "author": [],
      "contents": "\n\nSciSmile\n\n\nWith SciSmile, we aim to offer a scientifically-grounded, straightforward tool to promote psychological well-being and resilience in the face of stress.\n\n\n\n\n\n\nSciSmile is a smartphone application designed to improve well being and reduce stress through the power of smiling. SciSmile simulates reciprocal smiling interactions using videos of smiling people. The app tracks when users smile and triggers a responsive video of a person smiling back, thereby creating positive social feedback.\n\n\n\n\n\n\n\n\nCan it effectively reduce stress?\n\n\nWe launched the SciSmile app on both the Android and Apple App stores in 2021 to assess its appeal and effectiveness in the general population. The primary focus was to determine if such an intervention could effectively reduce stress levels and elevate mood. Preliminary results from our anonymous online pilot study in Switzerland suggest that a two-week training with SciSmile has strong potential for reducing stress and enhancing mood.\nOne year later, we launched SciMem, a simple smartphone memory game app. Interestingly, we found comparable effects.\nThus, it remains to be determined whether these beneficial effects are primarily the result of the apps or if they could be attributed to other factors such as placebo effects or regression to the mean. To answer this question, we are currently conducting a three-arm RCT to test the efficacy of SciSmile and SciMem, compared to a waitlist control. The trial is currently ongoing and preregistered on OSF.\n\n\n\n\n\nExample Screenshots from the SciSmile & SciMem App. Left: SciSmile; Right: SciMem.\n\n\n\n\n\n\n",
      "last_modified": "2024-06-02T20:36:29+02:00"
    }
  ],
  "collections": []
}
